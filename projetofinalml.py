# -*- coding: utf-8 -*-
"""ProjetoFinalML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y1Q_L-kmhuT6_p6M52XQj87Pd7had2F1

1. YOLOv8 - Treinamento e Inferência
"""

!pip install pytesseract ultralytics
!pip install -U transformers datasets accelerate

from ultralytics import YOLO
from google.colab import files
import os, zipfile, random, shutil, csv

# Upload e extração do dataset YOLO
uploaded = files.upload()
!unzip /content/anotacoes_projeto.v1i.yolov8.zip -d /content/cars-br

# Treinar YOLOv8
model = YOLO("yolov8n.yaml")
results = model.train(data="/content/cars-br/data.yaml", epochs=75, imgsz=640, pretrained=False)
model = YOLO("/content/runs/detect/train/weights/best.pt")

# Avaliação
metrics = model.val(data="/content/cars-br/data.yaml")
print(f"Map50-95: {metrics.box.map:.4f} | Map50: {metrics.box.map50:.4f} | Map75: {metrics.box.map75:.4f}")

# Inferência
results = model.predict(source="/content/cars-br/test/images", save=True, conf=0.25)

"""2. OCR - Organização, CSV e Fine-Tuning"""

# Upload do dataset de placas
uploaded = files.upload()
with zipfile.ZipFile("trOCR.zip", 'r') as zip_ref:
    zip_ref.extractall("anotacoes_ocr")

# Separação dos dados
base_dir = "/content/anotacoes_ocr/trOCR"
output_dir = "/content/split_data_ocr"
img_exts = ('.jpg', '.jpeg', '.png')
files = [f for f in os.listdir(base_dir) if f.lower().endswith(img_exts)]
paired = [f for f in files if os.path.exists(os.path.join(base_dir, f.rsplit('.', 1)[0] + '.txt'))]
random.shuffle(paired)

n = len(paired)
splits = {
    "train": paired[:int(n * 0.8)],
    "val": paired[int(n * 0.8):int(n * 0.9)],
    "test": paired[int(n * 0.9):]
}

for split in splits:
    os.makedirs(os.path.join(output_dir, split, "images"), exist_ok=True)
    os.makedirs(os.path.join(output_dir, split, "labels"), exist_ok=True)
    for f in splits[split]:
        shutil.copy2(os.path.join(base_dir, f), os.path.join(output_dir, split, "images", f))
        shutil.copy2(os.path.join(base_dir, f.replace(".jpg", ".txt")), os.path.join(output_dir, split, "labels", f.replace(".jpg", ".txt")))

# Gerar CSVs para OCR supervisionado
for split in ["train", "val", "test"]:
    folder_path = os.path.join(output_dir, split, "images")
    label_path = os.path.join(output_dir, split, "labels")
    csv_path = os.path.join(output_dir, f"{split}.csv")
    rows = []
    for file in os.listdir(folder_path):
        if file.lower().endswith(img_exts):
            txt_path = os.path.join(label_path, file.rsplit('.', 1)[0] + ".txt")
            if os.path.exists(txt_path):
                with open(txt_path, 'r') as f:
                    for line in f:
                        if line.lower().startswith("plate:"):
                            rows.append([os.path.join(folder_path, file), line.split("plate:")[1].strip()])
                            break
    with open(csv_path, 'w', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(["image_path", "text"])
        writer.writerows(rows)

"""3. OCR com TrOCR + Comparativo com o Tesseract + Métricas"""

from PIL import Image
import os
import cv2
import torch
import pytesseract
from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from ultralytics import YOLO

# 📦 Carregamento dos modelos
yolo_model = YOLO("/content/runs/detect/train/weights/best.pt")  # Modelo YOLOv8 treinado
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-base-printed")  # Tokenizador e pré-processador
model_trocr = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-base-printed").to("cuda" if torch.cuda.is_available() else "cpu")

# 💻 Define o dispositivo (CPU ou GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 📁 Diretórios de saída
output_crop_path = "/content/cropped_plates"
output_annotated_path = "/content/output_with_ocr"
os.makedirs(output_crop_path, exist_ok=True)
os.makedirs(output_annotated_path, exist_ok=True)

# 🧠 Função para detectar placas e aplicar OCR
def detect_and_trocr(image_path, save_crop=True, save_annotated=True):
    img = cv2.imread(image_path)
    results = yolo_model(img)[0]
    ocr_results = []

    if results.boxes is None or results.boxes.xyxy is None:
        return []

    for i, box in enumerate(results.boxes.xyxy.cpu().numpy()):
        x1, y1, x2, y2 = map(int, box.tolist())
        margin = 5
        x1 = max(x1 - margin, 0)
        y1 = max(y1 - margin, 0)
        x2 = min(x2 + margin, img.shape[1])
        y2 = min(y2 + margin, img.shape[0])

        cropped_plate = img[y1:y2, x1:x2]

        # --- OCR com TrOCR ---
        cropped_pil = Image.fromarray(cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2RGB)).convert("RGB")
        cropped_pil_resized = cropped_pil.resize((320, 80))
        pixel_values = processor(images=cropped_pil_resized, return_tensors="pt").pixel_values.to(device)
        with torch.no_grad():
            generated_ids = model_trocr.generate(pixel_values)
            trocr_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()

        # --- OCR com Tesseract ---
        gray = cv2.cvtColor(cropped_plate, cv2.COLOR_BGR2GRAY)
        thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]
        tesseract_text = pytesseract.image_to_string(thresh, config='--psm 7').strip()

        # Salva os resultados OCR
        ocr_results.append((x1, y1, x2, y2, trocr_text, tesseract_text))

        filename = os.path.basename(image_path).split('.')[0]
        if save_crop:
            cv2.imwrite(f"{output_crop_path}/{filename}_crop_{i}.jpg", cropped_plate)

        if save_annotated:
            annotated_text = f"T: {trocr_text} | S: {tesseract_text}"
            cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
            cv2.putText(img, annotated_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

    if save_annotated:
        cv2.imwrite(f"{output_annotated_path}/{os.path.basename(image_path)}", img)

    return ocr_results

images_path = "/content/split_data_ocr/test/images"
test_images = [f for f in os.listdir(images_path) if f.lower().endswith(('.jpg', '.png'))]

for img_name in test_images:
    img_path = os.path.join(images_path, img_name)
    results = detect_and_trocr(img_path)
    print(f"\n🔍 {img_name}")
    if results:
        for box in results:
            print(f"📌 BBox: {box[:4]}")
            print(f"🧠 TrOCR: {box[4]}")
            print(f"📝 Tesseract: {box[5]}")
    else:
        print("Nenhuma placa detectada.")

!pip install python-Levenshtein

import os
import csv

# Diretórios
test_images_dir = "/content/split_data_ocr/test/images"
test_labels_dir = "/content/split_data_ocr/test/labels"

# 1. Função para limpar texto
def clean_plate_text(text):
    if text is None:
        return ""
    return ''.join(c for c in text.upper() if c.isalnum())

# 2. Ler ground truth dos arquivos .txt
ground_truth = {}
for txt_file in os.listdir(test_labels_dir):
    if txt_file.endswith(".txt"):
        img_name = txt_file.replace(".txt", ".jpg")  # ajuste conforme extensão real
        with open(os.path.join(test_labels_dir, txt_file), "r") as f:
            for line in f:
                if line.lower().startswith("plate:"):
                    plate = line.split("plate:")[1].strip()
                    ground_truth[img_name] = clean_plate_text(plate)
                    break

# 3. Rodar OCR para todas imagens de teste, salvar resultados num dicionário
predicted = {}
for img_name in os.listdir(test_images_dir):
    if not img_name.lower().endswith(('.jpg','.jpeg','.png')):
        continue
    img_path = os.path.join(test_images_dir, img_name)
    ocr_results = detect_and_trocr(img_path, save_crop=False, save_annotated=False)

    # Caso haja múltiplas detecções, considere só a primeira placa
    if ocr_results:
        trocr_text = clean_plate_text(ocr_results[0][4])  # texto do TrOCR
        predicted[img_name] = trocr_text
    else:
        predicted[img_name] = ""  # Nenhuma placa detectada

# 4. Avaliação: comparar predicted vs ground_truth

from sklearn.metrics import accuracy_score
import Levenshtein

imgs = list(ground_truth.keys())
gt_clean = [ground_truth[img] for img in imgs]
pred_clean = [predicted.get(img, "") for img in imgs]

acc = accuracy_score(gt_clean, pred_clean)
print(f"Acurácia exata: {acc:.2%}")

def normalized_levenshtein(s1, s2):
    if len(s1) == 0 and len(s2) == 0:
        return 1.0
    dist = Levenshtein.distance(s1, s2)
    max_len = max(len(s1), len(s2))
    return 1 - dist / max_len

similarities = [normalized_levenshtein(g, p) for g, p in zip(gt_clean, pred_clean)]
avg_similarity = sum(similarities) / len(similarities)
print(f"Similaridade média (Levenshtein normalizada): {avg_similarity:.2%}")

print("\nErros encontrados:")
for img, gt, pred in zip(imgs, gt_clean, pred_clean):
    if gt != pred:
        print(f"Erro na {img}: esperado='{gt}', previsto='{pred}'")

from collections import Counter

def character_confusion_matrix(gt_list, pred_list):
    confusion = Counter()

    for gt, pred in zip(gt_list, pred_list):
        # Usar o tamanho mínimo para comparar
        length = min(len(gt), len(pred))
        for i in range(length):
            if gt[i] != pred[i]:
                confusion[(gt[i], pred[i])] += 1
        # Caso gt maior que pred, considerar erros de omissão
        for c in gt[length:]:
            confusion[(c, None)] += 1
        # Caso pred maior que gt, considerar erros de inserção
        for c in pred[length:]:
            confusion[(None, c)] += 1
    return confusion

# Exemplo de uso
confusions = character_confusion_matrix(gt_clean, pred_clean)

# Mostrar os pares mais comuns
print("Principais confusões de caracteres:")
for (c1, c2), count in confusions.most_common(10):
    print(f"'{c1}' -> '{c2}' : {count} vezes")

def clean_plate_text(text):
    if text is None:
        return ""
    text = text.upper()
    # Remover símbolos comuns
    for ch in ['#', '-', ' ', ',', '.', '(', ')', '[', ']', '_', ':']:
        text = text.replace(ch, '')
    # Substituir caracteres confusos (exemplo)
    substitutions = {
        '0': 'O',  # só se fizer sentido no contexto da placa
        '1': 'I',
        '5': 'S',
        '8': 'B',
        # ajuste conforme padrão da sua base
    }
    for k, v in substitutions.items():
        text = text.replace(k, v)
    # Manter só letras e números
    text = ''.join(c for c in text if c.isalnum())
    return text

import re

def correct_plate_format(text):
    # Exemplo: tentar extrair 3 letras e 4 números
    match = re.match(r'([A-Z]{3})(\d{4})', text)
    if match:
        return match.group(1) + match.group(2)
    else:
        # Tentar corrigir com regex aproximado, ou retornar texto original
        return text

# Limpar textos com substituições
gt_clean = [clean_plate_text(ground_truth.get(img, "")) for img in ground_truth.keys()]
pred_clean = [clean_plate_text(predicted.get(img, "")) for img in ground_truth.keys()]

# Corrigir formato
gt_clean = [correct_plate_format(t) for t in gt_clean]
pred_clean = [correct_plate_format(t) for t in pred_clean]

# Calcular métricas
acc = accuracy_score(gt_clean, pred_clean)
print(f"Acurácia exata corrigida: {acc:.2%}")

# Matriz de confusão
confusions = character_confusion_matrix(gt_clean, pred_clean)
print("Principais confusões de caracteres:")
for (c1, c2), count in confusions.most_common(10):
    print(f"'{c1}' -> '{c2}' : {count} vezes")

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict

def build_confusion_matrix_chars(gt_list, pred_list):
    chars = set()
    # Primeiro, pegar todos os caracteres únicos do conjunto
    for text in gt_list + pred_list:
        for c in text:
            chars.add(c)
    chars = sorted(chars)

    # Mapeamento char -> índice
    char_to_idx = {c: i for i, c in enumerate(chars)}

    # Inicializar matriz de confusão
    matrix = np.zeros((len(chars), len(chars)), dtype=int)

    for gt, pred in zip(gt_list, pred_list):
        length = min(len(gt), len(pred))
        for i in range(length):
            gt_c = gt[i]
            pred_c = pred[i]
            matrix[char_to_idx[gt_c], char_to_idx[pred_c]] += 1
        # Considerar erros de omissão (char esperado, mas não previsto)
        for c in gt[length:]:
            matrix[char_to_idx[c], :] += 1  # conta como erro geral (pode adaptar)
        # Considerar erros de inserção (char previsto, mas não esperado)
        for c in pred[length:]:
            matrix[:, char_to_idx[c]] += 1  # conta como erro geral (pode adaptar)

    return matrix, chars

# Exemplo de uso com listas já limpas (gt_clean e pred_clean)
matrix, labels = build_confusion_matrix_chars(gt_clean, pred_clean)

# Visualizar com heatmap
plt.figure(figsize=(12,10))
sns.heatmap(matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels, cmap='Blues')
plt.xlabel('Caracteres previstos')
plt.ylabel('Caracteres reais')
plt.title('Matriz de Confusão de Caracteres do OCR')
plt.show()















































































































